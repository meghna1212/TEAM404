{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Parsing and Summarization using Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is regarded as the fastest NLP framework in Python, with single optimized functions for each of the NLP tasks it implements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('C:\\\\Users\\\\Meghna\\\\Desktop\\\\CV Ranking\\\\train_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/',\n",
       " {'entities': [(1749, 1755, 'Companies worked at'),\n",
       "   (1696, 1702, 'Companies worked at'),\n",
       "   (1417, 1423, 'Companies worked at'),\n",
       "   (1356, 1793, 'Skills'),\n",
       "   (1209, 1215, 'Companies worked at'),\n",
       "   (1136, 1248, 'Skills'),\n",
       "   (928, 932, 'Graduation Year'),\n",
       "   (858, 889, 'College Name'),\n",
       "   (821, 856, 'Degree'),\n",
       "   (787, 791, 'Graduation Year'),\n",
       "   (744, 750, 'Companies worked at'),\n",
       "   (722, 742, 'Designation'),\n",
       "   (658, 664, 'Companies worked at'),\n",
       "   (640, 656, 'Designation'),\n",
       "   (574, 580, 'Companies worked at'),\n",
       "   (555, 573, 'Designation'),\n",
       "   (470, 493, 'Companies worked at'),\n",
       "   (444, 469, 'Designation'),\n",
       "   (308, 314, 'Companies worked at'),\n",
       "   (234, 240, 'Companies worked at'),\n",
       "   (175, 198, 'Companies worked at'),\n",
       "   (93, 137, 'Email Address'),\n",
       "   (39, 48, 'Location'),\n",
       "   (13, 38, 'Designation'),\n",
       "   (0, 12, 'Name')]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of the training data is a tuple. The tuple consists of two parts. \n",
    "\n",
    "The first part is the complete textual data of the CV which is having a data type of string.\n",
    "The second part is a dictionary. The dictionary contains a list as the value and this is a list of tuples. The last element of each tuple is the LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure that we are following to parse the CV is first convert all of the data in a CV in PDF format to text format. \n",
    "\n",
    "Next, we have to determine all of the requried entites in the CV(like College Name, Degree, Skills etc.) and the position from where that entitiy begins and ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a standard NLP problem which involves spotting named entities (people, places, organizations etc.) from a chunk of text, and classifying them into a predefined set of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "def train_model(train_data):\n",
    "    if 'ner' not in nlp.pipe_names:     \n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    \n",
    "    #We are checking whether the NER is already present in the pipeline list or not\n",
    "    #If not, we add it to the end of the pipeline list\n",
    "    \n",
    "    for _, annotation in train_data:\n",
    "        #annotation is the second half of each training data tuple\n",
    "        for ent in annotation['entities']:\n",
    "            #Here, we are adding all the NER labels from the CV PDF into the dictionary\n",
    "            #At position, index 2\n",
    "            ner.add_label(ent[2])\n",
    "     \n",
    "    #Now we are going to train our model only on the NERs in our PDFs and will put all of the other NERs\n",
    "    #into other_pipes list\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):     #Only train on required NERs\n",
    "        optimizer = nlp.begin_training()\n",
    "        \n",
    "        for itn in range (10):                #We shall be performing training for 10 iterations\n",
    "            print('Starting iteration ', itn)\n",
    "            random.shuffle(train_data)        #By shuffling the training data\n",
    "            losses = {}\n",
    "            index = 0\n",
    "            for text, annotations in train_data:\n",
    "                try:\n",
    "                    nlp.update([text],\n",
    "                              [annotations],\n",
    "                              drop=0.2,\n",
    "                              sgd=optimizer,\n",
    "                              losses=losses)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    \n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration  0\n",
      "{'ner': 14199.331548548138}\n",
      "Starting iteration  1\n",
      "{'ner': 8488.050956346422}\n",
      "Starting iteration  2\n",
      "{'ner': 8431.342042162381}\n",
      "Starting iteration  3\n",
      "{'ner': 8252.535296776401}\n",
      "Starting iteration  4\n",
      "{'ner': 6858.541863939513}\n",
      "Starting iteration  5\n",
      "{'ner': 7132.293477769844}\n",
      "Starting iteration  6\n",
      "{'ner': 5196.588650850168}\n",
      "Starting iteration  7\n",
      "{'ner': 5287.0299678735755}\n",
      "Starting iteration  8\n",
      "{'ner': 7309.475048217748}\n",
      "Starting iteration  9\n",
      "{'ner': 4833.048514014466}\n"
     ]
    }
   ],
   "source": [
    "train_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now save this trained NLP model to our local disk\n",
    "nlp.to_disk('C:\\\\Users\\\\Meghna\\\\Desktop\\\\CV Ranking\\\\nlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained model into spacy \n",
    "nlp_model = spacy.load('C:\\\\Users\\\\Meghna\\\\Desktop\\\\CV Ranking\\\\nlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Shreya Agnihotri Senior System Engineer at Infosys Limited - Infosys  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Shreya-Agnihotri/ c1755567027a0205  • Having 2.7 years of experience in Web Application design using python Django framework. • Highly experienced and skilled Agile Developer with a strong record of excellent teamwork and successful coding project management. • Good knowledge in python, Elasticsearch, Django using HTML5, MYSQL, JavaScript, jQuery. • Performed the role of team member effectively. Involved in requirement gathering and analysis of the requirements in technical perspective. • Extensively worked on software in all the phases including Design, Development, Implementation, Integration and Testing. • Possesses good analytical, logical ability and systematic approach to problem analysis, strong debugging and troubleshooting skills. • Working on classic software development models along Agile Methodologies.  WORK EXPERIENCE  Senior System Engineer at Infosys Limited  Infosys -  January 2016 to Present  training program. • Working as Senior System Engineer at Infosys Limited from January 2016 to till date.  EDUCATION  B.Tech in ECE  Galgotias University  SKILLS  Ajax (Less than 1 year), APACHE KAFKA (Less than 1 year), HTML5 (2 years), Java (2 years), SQL (2 years)  ADDITIONAL INFORMATION  Technical Profile:  Web Technologies Python, JAVA, HTML5.  Frameworks Django framework.  https://www.indeed.com/r/Shreya-Agnihotri/c1755567027a0205?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Shreya-Agnihotri/c1755567027a0205?isid=rex-download&ikw=download-top&co=IN   Programming Languages Python, Java.  Scripting Language jQuery, JavaScript, Ajax.  Operating Systems: Linux  Databases: SQL, kafka.  Tools and Utilities: Elasticsearch, Prometheus, Grafana, kibana, Docker',\n",
       " {'entities': [(1632, 1836, 'Skills'),\n",
       "   (1211, 1399, 'Skills'),\n",
       "   (1181, 1202, 'College Name'),\n",
       "   (1166, 1179, 'Degree'),\n",
       "   (1105, 1113, 'Companies worked at'),\n",
       "   (1080, 1121, 'Designation'),\n",
       "   (995, 1003, 'Companies worked at'),\n",
       "   (970, 1011, 'Designation'),\n",
       "   (113, 161, 'Email Address'),\n",
       "   (70, 79, 'Location'),\n",
       "   (60, 68, 'Companies worked at'),\n",
       "   (42, 50, 'Companies worked at'),\n",
       "   (17, 58, 'Designation'),\n",
       "   (0, 16, 'Name')]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since we shuffled out training dataset, hence the first element is different now\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's not a good idea to test our model on training data, but still lets see whether our model is working correctly or not by applying the model on a training dataset data.\n",
    "\n",
    "So, we'll only be pasing the text to our model and check if we are getting the corresponding entities or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                          -Shreya Agnihotri\n",
      "DESIGNATION                   -Senior System Engineer\n",
      "COMPANIES WORKED AT           -Infosys Limited\n",
      "LOCATION                      -Bengaluru\n",
      "EMAIL ADDRESS                 -indeed.com/r/Shreya-Agnihotri/ c1755567027a0205\n",
      "DESIGNATION                   -Senior System Engineer\n",
      "DEGREE                        -B.Tech in ECE\n",
      "COLLEGE NAME                  -Galgotias University\n",
      "SKILLS                        -Ajax (Less than 1 year), APACHE KAFKA (Less than 1 year), HTML5 (2 years), Java (2 years), SQL (2 years)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_model(train_data[0][0])\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.label_.upper():{30}}-{ent.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets test the model on a random CV PDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first we need to extract the data from the PDF and then pass it through the model to get classification of text into different entites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice Clark \\nAI / Machine Learning \\n \\nDelhi, India Email me on Indeed \\n• \\n20+ years of experience in data handling, design, and development \\n• \\nData Warehouse: Data analysis, star/snow flake scema data modelling and design specific to \\ndata warehousing and business intelligence \\n• \\nDatabase: Experience in database designing, scalability, back-up and recovery, writing and \\noptimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. \\nCloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, \\nStream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake \\nanalytics(U-SQL) \\nWilling to relocate anywhere \\n \\nWORK EXPERIENCE \\nSoftware Engineer \\nMicrosoft – Bangalore, Karnataka \\nJanuary 2000 to Present \\n1. Microsoft Rewards Live dashboards: \\nDescription: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping \\nonline. Microsoft Rewards members can earn points when searching with Bing, browsing with \\nMicrosoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft \\nStore. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft \\nrewards website. Rewards live dashboards gives a live picture of usage world-wide and by \\nmarkets like US, Canada, Australia, new user registration count, top/bottom performing rewards \\noffers, orders stats and weekly trends of user activities, orders and new user registrations. the \\nPBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. \\nTechnology/Tools used \\n \\nEDUCATION \\nIndian Institute of Technology – Mumbai \\n2001 \\n \\nSKILLS \\nMachine Learning, Natural Language Processing, and Big Data Handling \\n \\nADDITIONAL INFORMATION \\nProfessional Skills \\n• Excellent analytical, problem solving, communication, knowledge transfer and interpersonal \\nskills with ability to interact with individuals at all the levels \\n• Quick learner and maintains cordial relationship with project manager and team members and \\ngood performer both in team and independent job environments \\n• Positive attitude towards superiors &amp; peers \\n• Supervised junior developers throughout project lifecycle and provided technical assistance \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, fitz\n",
    "fname = 'C:\\\\Users\\\\Meghna\\\\Desktop\\\\CV Ranking\\\\Alice Clark CV.pdf'\n",
    "doc = fitz.open(fname)\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text = text + str(page.get_text())\n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice Clark \n",
      "AI / Machine Learning \n",
      " \n",
      "Delhi, India Email me on Indeed \n",
      "• \n",
      "20+ years of experience in data handling, design, and development \n",
      "• \n",
      "Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to \n",
      "data warehousing and business intelligence \n",
      "• \n",
      "Database: Experience in database designing, scalability, back-up and recovery, writing and \n",
      "optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. \n",
      "Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, \n",
      "Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake \n",
      "analytics(U-SQL) \n",
      "Willing to relocate anywhere \n",
      " \n",
      "WORK EXPERIENCE \n",
      "Software Engineer \n",
      "Microsoft – Bangalore, Karnataka \n",
      "January 2000 to Present \n",
      "1. Microsoft Rewards Live dashboards: \n",
      "Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping \n",
      "online. Microsoft Rewards members can earn points when searching with Bing, browsing with \n",
      "Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft \n",
      "Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft \n",
      "rewards website. Rewards live dashboards gives a live picture of usage world-wide and by \n",
      "markets like US, Canada, Australia, new user registration count, top/bottom performing rewards \n",
      "offers, orders stats and weekly trends of user activities, orders and new user registrations. the \n",
      "PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. \n",
      "Technology/Tools used \n",
      " \n",
      "EDUCATION \n",
      "Indian Institute of Technology – Mumbai \n",
      "2001 \n",
      " \n",
      "SKILLS \n",
      "Machine Learning, Natural Language Processing, and Big Data Handling \n",
      " \n",
      "ADDITIONAL INFORMATION \n",
      "Professional Skills \n",
      "• Excellent analytical, problem solving, communication, knowledge transfer and interpersonal \n",
      "skills with ability to interact with individuals at all the levels \n",
      "• Quick learner and maintains cordial relationship with project manager and team members and \n",
      "good performer both in team and independent job environments \n",
      "• Positive attitude towards superiors &amp; peers \n",
      "• Supervised junior developers throughout project lifecycle and provided technical assistance \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are many special characters and new lines, so we need to remove these extra characters and obtain clean data. We'll do that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice Clark  AI / Machine Learning    Delhi, India Email me on Indeed  •  20+ years of experience in data handling, design, and development  •  Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to  data warehousing and business intelligence  •  Database: Experience in database designing, scalability, back-up and recovery, writing and  optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes.  Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure,  Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake  analytics(U-SQL)  Willing to relocate anywhere    WORK EXPERIENCE  Software Engineer  Microsoft – Bangalore, Karnataka  January 2000 to Present  1. Microsoft Rewards Live dashboards:  Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping  online. Microsoft Rewards members can earn points when searching with Bing, browsing with  Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft  Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft  rewards website. Rewards live dashboards gives a live picture of usage world-wide and by  markets like US, Canada, Australia, new user registration count, top/bottom performing rewards  offers, orders stats and weekly trends of user activities, orders and new user registrations. the  PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes.  Technology/Tools used    EDUCATION  Indian Institute of Technology – Mumbai  2001    SKILLS  Machine Learning, Natural Language Processing, and Big Data Handling    ADDITIONAL INFORMATION  Professional Skills  • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal  skills with ability to interact with individuals at all the levels  • Quick learner and maintains cordial relationship with project manager and team members and  good performer both in team and independent job environments  • Positive attitude towards superiors &amp; peers  • Supervised junior developers throughout project lifecycle and provided technical assistance  \n"
     ]
    }
   ],
   "source": [
    "clean_text = \" \".join(text.split('\\n'))\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll test our model on this cleaned and processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                          -Alice Clark\n",
      "LOCATION                      -AI /\n",
      "LOCATION                      -Delhi\n",
      "DESIGNATION                   -Software Engineer\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "DEGREE                        -Indian Institute of Technology\n",
      "SKILLS                        -Machine Learning, Natural Language Processing, and Big Data Handling    ADDITIONAL INFORMATION  Professional Skills  • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal  skills with ability to interact with individuals at all the levels  • Quick learner and maintains cordial relationship with project manager and team members and  good performer both in team and independent job environments  • Positive attitude towards superiors &amp; peers  • Supervised junior developers throughout project lifecycle and provided technical assistance\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_model(clean_text)\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.label_.upper():{30}}-{ent.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model on another CV PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Smith  BI / Big Data/ Azure  Manchester, UK- Email me on Indeed: indeed.com/r/falicent/140749dace5dc26f    10+ years of Experience in Designing, Development, Administration, Analysis,  Management  inthe  Business  Intelligence  Data  warehousing,  Client  Server  Technologies, Web-based Applications, cloud solutions and Databases.  Data warehouse: Data analysis, star/ snow flake schema data modeling and design  specific todata warehousing and business intelligence environment.  Database: Experience in database designing, scalability, back-up and recovery,  writing andoptimizing SQL code and Stored Procedures, creating functions, views,  triggers and indexes.   Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL  Azure, StreamAnalytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure  data lake analytics(U-SQL).  Big Data: Worked Azure data lake store/analytics for big data processing and Azure  data factoryto schedule U-SQL jobs. Designed and developed end to end big data  solution for data insights.     Willing to relocate: Anywhere  WORK EXPERIENCESoftware Engineer  Microsoft - Manchester, UK.  December 2015 to Present  1. Microsoft Rewards Live dashboards:  Description: - Microsoft rewards is loyalty program that rewards Users for  browsing and shopping online. Microsoft Rewards members can earn points when  searching with Bing, browsing with Microsoft Edge and making purchases at the  Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up  bonus points for taking daily quizzes and tours on the Microsoft rewards website.  Rewards live dashboards gives a live picture of usage world-wide and by markets  like US, Canada, Australia, new user registration count, top/bottom performing  rewards offers, orders stats and weekly trends of user activities, orders and new  user registrations. the PBI tiles gets refreshed in different frequencies starting  from 5 seconds to 30 minutes.  Technology/Tools used  Event hub, stream analytics and Power BI.  Responsibilities  Created stream analytics jobs to process event hub data  Created Power BI live dashboard to show live usage traffic, weekly trends, cards,  charts to showtop/bottom 10 offers and usage metrics.  2. Microsoft Rewards Data Insights:  Description: - Microsoft rewards is loyalty program that rewards Users for  browsing and shopping online. Microsoft Rewards members can earn points when  searching with Bing, browsing with Microsoft Edge and making purchases at the  Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up  bonus points for taking daily quizzes and tours on the Microsoft rewards website.  Rewards data insights is data analytics and reporting platform, processes 20  million users daily activities and redemption across different markets like US,  Canada, Australia.  Technology/Tools used  Cosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI.  Responsibilities  Created big data scripts in cosmos  C# data extractors, processors and reducers for data transformation  Power BI dashboards  3. End to end tracking Tool:  Description: - This is real-time Tracking tool to track different business  transactions like order, order response, functional acknowledgement, invoice  flowing inside ICOE. It gives flexibility to customers to track their transactions  and appropriate error information in-case of any failure. Based on resource based  access control the tool gives flexibility to end user to perform different actions  like view transactions, search based on different filter criteria and view and  download actual message payload. End to end tracking tool stitches all the  business transaction like order to cash flow and connects different hops inside  ICOE like gateway, routing server, Processing server. It also connects different  systems like ICOE, partner end point and SAP.  Technology/Tools used  Azure Document db, Azure web job and Web APP, RBAC, Angular JS.  Responsibilities  Document dB stored procedures.  Web job to process event hub data and populate Document db• Web App API.  Stream analytics job to transform data  Power BI reports  4. Biztrack Tracking Tool:  Description: - This is real-time Tracking tool to track different business  transactions like order, order response, functional acknowledgement, invoice  flowing inside ICOE. It gives flexibility to customers to track their transactions  and appropriate error information in-case of any failure. Based on resource based  access control the tool gives flexibility to end user to perform different actions  like view transactions, search based on different filter criteria and view and  download actual message payload.  Technology/Tools used  SQL server 2014, SSIS, .net API, Angular JS.  Responsibilities  ETL solution to transform business transactions data stored in Biztalk tables.  SQL azure tables, stored procedures, User defined functions.  Performance tuning.  Web API enhancements.    EDUCATION  The University of Manchester - UK  2007    SKILLS  problem solving (Less than 1 year), project lifecycle (Less than 1 year), project  manager (Less than 1 year), technical assistance. (Less than 1 year)  ADDITIONAL INFORMATION  Professional Skills  Excellent analytical, problem solving, communication, knowledge transfer and  interpersonalskills with ability to interact with individuals at all the levels  Quick learner and maintains cordial relationship with project manager and team  members andgood performer both in team and independent job environments  Positive attitude towards superiors &amp; peers  Supervised junior developers throughout project lifecycle and provided technical  assistance.  \n"
     ]
    }
   ],
   "source": [
    "import sys, fitz\n",
    "fname = 'C:\\\\Users\\\\Meghna\\\\Desktop\\\\CV Ranking\\\\Smith Resume.pdf'\n",
    "doc = fitz.open(fname)\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text = text + str(page.get_text())\n",
    "    \n",
    "clean_text = \" \".join(text.split('\\n'))\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                          -Michael Smith\n",
      "DESIGNATION                   -Big Data/ Azure\n",
      "EMAIL ADDRESS                 -indeed.com/r/falicent/140749dace5dc26f\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COMPANIES WORKED AT           -Microsoft\n",
      "COLLEGE NAME                  -The University of Manchester - UK  \n",
      "SKILLS                        -problem solving (Less than 1 year), project lifecycle (Less than 1 year), project\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_model(clean_text)\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.label_.upper():{30}}-{ent.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, our model works and is efficiently able to extract all of the relevant information from the PDF CVs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
